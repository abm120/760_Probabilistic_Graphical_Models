{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECEN760 Assignment-2: Text Classification using Multinomial Naive Bayes\n",
    "\n",
    "   **Name:**  \"Anil B Murthy\"                      \n",
    "   \n",
    "   **UIN: **  \"525006147\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this assignment, you will write a python program to classify texts using a Multinomial Naive Bayes classifier. \n",
    "\n",
    "* Since this is your first programming assignment using python, you're provided with the sketch of the program. You only need to complete the cells that says \" # Write your code here \". \n",
    "\n",
    "* Feel free to organize your code into multiple cells for better readability. \n",
    "* You may also create sub-functions/ definitions.\n",
    "* You may also import more standard libraries in addition to the libraries that are already imported in the preprocessing part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "**20 Newsgroups** ( http://qwone.com/~jason/20Newsgroups/ )\n",
    "\n",
    "The dataset has approximately 18,000 Newsgroups documents on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\n",
    "\n",
    "For this assignment, we will only use documents from 3 categories ('talk.religion.misc','comp.graphics','sci.space').\n",
    "\n",
    "We recommend you to take advantage of the scikit-learn library to import and process the dataset. More information can be found in the following link:\n",
    "\n",
    "   http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing\n",
    "\n",
    "In this step, we'll download the dataset using the scikit-learn library. The sample program to import the library is given below.\n",
    "\n",
    "This involves two steps:\n",
    "\n",
    "    1) Fetch dataset corresponding to the three following categories:\n",
    "\n",
    "    *  'talk.religion.misc'\n",
    "    *  'comp.graphics'\n",
    "    *  'sci.space'\n",
    "    \n",
    "    2) Remove stop words* and create count vectors for the train and test datasets. \n",
    "    \n",
    "\n",
    "    *Stop words in this context refer to words that occur very frequently and thus contain little information about the type of the article itself.  For e.g., 'a','and','the' etc. See https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py for the list of stop words used in scikit when 'english' is given as input.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "\n",
    "# Feel free to import any standard libraries that you may need to complete the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1: ** Fetch the dataset for the three aforementioned categories using scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc','comp.graphics','sci.space']\n",
    "\n",
    "num_categories = len(categories)\n",
    "\n",
    "#Loading training data\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "#Loading testing data\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Loading the class labels for training and testing data\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contatins \n",
      " 1554 train documents, \n",
      " 1034 test documents.\n"
     ]
    }
   ],
   "source": [
    "# Total number of documents in train and test datasets\n",
    "\n",
    "num_train = len(data_train.target)\n",
    "num_test = len(data_test.target)\n",
    "\n",
    "print(\"Dataset contatins \\n \"\n",
    "       +str(num_train)+\" train documents, \\n \"\n",
    "       + str(num_test) + \" test documents.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print a sample document to understand the dataset better.\n",
    "\n",
    "Fill in the cell below to print contents of the first document from \"train\" subset. Also, print its corresponding class label name(category).\n",
    "\n",
    "**Hint:** Use \"data_train.data\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train Document:\n",
      "\n",
      "From: nicho@vnet.IBM.COM (Greg Stewart-Nicholls)\n",
      "Subject: Re: Biosphere II\n",
      "Reply-To: nicho@vnet.ibm.com\n",
      "Disclaimer: This posting represents the poster's views, not those of IBM\n",
      "News-Software: UReply 3.1\n",
      "X-X-From: nicho@vnet.ibm.com\n",
      "            <1q1kia$gg8@access.digex.net>\n",
      "Lines: 18\n",
      "\n",
      "In <1q1kia$gg8@access.digex.net> Pat writes:\n",
      ">In article <19930408.043740.516@almaden.ibm.com> nicho@vnet.ibm.com writes:\n",
      ">>In <1q09ud$ji0@access.digex.net> Pat writes:\n",
      ">>>Why is everyone being so critical of B2?\n",
      ">> Because it's bogus science, promoted as 'real' science.\n",
      ">It seems to me, that it's sorta a large engineering project more\n",
      ">then a science project.\n",
      "  Bingo.\n",
      ">B2 is not bench science,  but rather a large scale attempt to\n",
      ">re-create a series of micro-ecologies.   what's so eveil about this?\n",
      " Nothing evil at all. There's no actual harm in what they're doing, only\n",
      "how they represent it.\n",
      "\n",
      " -----------------------------------------------------------------\n",
      " .sig files are like strings ... every yo-yo's got one.\n",
      "\n",
      "Greg Nicholls ... nicho@vnet.ibm.com (business) or\n",
      "                  nicho@olympus.demon.co.uk (private)\n",
      "\n",
      "\n",
      "\n",
      " The above document belongs to the following category:\t\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample train Document:\\n\")\n",
    "print(data_train.data[0])\n",
    "print(\"\\n\\n The above document belongs to the following category:\\t\")\n",
    "print(categories[data_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2:** Remove stop words and create count vectors for the train and test datasets.\n",
    "\n",
    "   We use the CountVectorizer method to extract features (counts for each word). Note that words from both training and testing data are needed to build the count table.\n",
    "   \n",
    "   *Documentation:*  http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in vocabulary = 37830\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(data_train.data + data_test.data)\n",
    "x_train = vectorizer.transform(data_train.data)\n",
    "x_test = vectorizer.transform(data_test.data)\n",
    "vocab_length = len(vectorizer.vocabulary_)\n",
    "print(\"Total number of words in vocabulary = \"+str(vocab_length))\n",
    "#print(x_test.getrow(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a Multinomial Naive Bayes classifier that takes feature vector from the test data as input and classifies as one of the three classes ('talk.religion.misc','comp.graphics','sci.space').\n",
    "\n",
    "Complete the training function MultiNB_train() in the cell below to train a Multiomial Naive Bayes classifier that takes \"x_train\",\"y_train\",\"alpha\" as inputs and returns the likelihood probability matrix \"theta\" and the prior distribution  \"prior\" on the document category.\n",
    "\n",
    "\"prior\" is a vector of length equal to num_categories where the $i$-th element is defined as\n",
    "$$ prior (i) = \\frac{\\text{ # of train documents with category i}}{\\text{Total number of train documets}} $$\n",
    "\n",
    "\"theta\" ($\\theta$) is the  matrix with the $(c,i)$th element defined by\n",
    "\n",
    " $$ \\theta(c,i) = P(w_i/c) =  \\frac{N_{ci} + \\alpha }{N_c + |V| \\alpha}$$\n",
    " \n",
    " where,\n",
    " * $P(w_i/c)$ refers to the probability of seeing the $i$th word in the vocabulary given that class type is $c$.\n",
    " * $N_{ci}$ refers to the total number of times the word  $i$ appeared in the training documents of class type $c$.\n",
    " * $N_c$ is the total number of words in the documents of type $c$\n",
    "    $$N_c = \\sum_{d \\in T[c]} N_{cd}$$\n",
    "    where, $T[c]$ refers to the documents of type $c$.\n",
    " * $|V|$ is the size of the vocabulary.\n",
    " * $\\alpha$ is the laplace smoothing parameter\n",
    "\n",
    "***Note**: **Do NOT** use the scikit-learn's inbuilt function \"MultinomialNB\" . Write your own code to build the classifier. You may use standary libraries like \"numpy\",\"scipy\" etc. to perform operations on matrices/arrays. \n",
    "\n",
    "Feel free to break your code into multiple functions or cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultiNB_train(x_train,y_train, alpha):\n",
    "    #alpha = 0.01\n",
    "    count = [0,0,0]\n",
    "    for i in range(len(y_train)):\n",
    "        if(y_train[i] == 0):\n",
    "            count[0] = count[0] + 1\n",
    "        elif(y_train[i] == 1):\n",
    "            count[1] = count[1] + 1\n",
    "        elif(y_train[i] == 2):\n",
    "            count[2] = count[2] + 1\n",
    "    prior = [(count[0]/float(num_train)),(count[1]/float(num_train)),(count[2]/float(num_train))]\n",
    "    print(prior)\n",
    "    print(x_train.shape)\n",
    "    x_train_list = [((i, j), x_train[i,j]) for i, j in zip(*x_train.nonzero())] #converts CSR matrix format to List of tuples\n",
    "    print(x_train_list[0])\n",
    "    len_x_train = len(x_train_list)\n",
    "    print(len_x_train)\n",
    "    Nc = [0,0,0]\n",
    "    N0i = [0] * vocab_length\n",
    "    N1i = [0] * vocab_length\n",
    "    N2i = [0] * vocab_length\n",
    "\n",
    "    for i in range(len_x_train):\n",
    "        category = data_train.target[x_train_list[i][0][0]]\n",
    "        if(category == 0):\n",
    "            Nc[0] = Nc[0] + x_train_list[i][1]\n",
    "            N0i[x_train_list[i][0][1]] += x_train_list[i][1]\n",
    "        elif(category == 1):\n",
    "            Nc[1] = Nc[1] + x_train_list[i][1]\n",
    "            N1i[x_train_list[i][0][1]] += x_train_list[i][1]\n",
    "        elif(category == 2):\n",
    "            Nc[2] = Nc[2] + x_train_list[i][1]\n",
    "            N2i[x_train_list[i][0][1]] += x_train_list[i][1]\n",
    "\n",
    "    theta0 = [((x + alpha)/float(Nc[0] + (vocab_length*alpha))) for x in N0i]\n",
    "    theta1 = [((x + alpha)/float(Nc[1] + (vocab_length*alpha))) for x in N1i]\n",
    "    theta2 = [((x + alpha)/float(Nc[2] + (vocab_length*alpha))) for x in N2i]\n",
    "\n",
    "    theta = [theta0, theta1, theta2]\n",
    "    print(len(theta0))\n",
    "    print(len(theta1))\n",
    "    print(len(theta2))\n",
    "    print(len(theta))\n",
    "    print(theta[0][9])\n",
    "\n",
    "    return(theta, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model to learn the likelihood parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3758043758043758, 0.3815958815958816, 0.2425997425997426]\n",
      "(1554, 37830)\n",
      "((0, 192), 1)\n",
      "175894\n",
      "37830\n",
      "37830\n",
      "37830\n",
      "3\n",
      "2.33655778307e-05\n"
     ]
    }
   ],
   "source": [
    "theta,prior = MultiNB_train(x_train,y_train,alpha = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the classifier function MultiNB_classify() below that takes in features of one test sample (one row from x_test) and returns the predicted class \"pred_class\" $\\in \\{0,1,2\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultiNB_classify(x_test_sample, theta, prior):\n",
    "    category0 = 0\n",
    "    category1 = 0\n",
    "    category2 = 0\n",
    "    sample_list = [((i, j), x_test_sample[i,j]) for i, j in zip(*x_test_sample.nonzero())]\n",
    "    sample_len = len(sample_list)\n",
    "    for i in range(sample_len):\n",
    "        word_no = sample_list[i][0][1]\n",
    "        word_count = sample_list[i][1]\n",
    "        max_prob = max(theta[0][word_no], theta[1][word_no], theta[2][word_no])\n",
    "        if(max_prob == theta[0][word_no]):\n",
    "            category0 += 1\n",
    "        elif(max_prob == theta[1][word_no]):\n",
    "            category1 += 1\n",
    "        else:\n",
    "            category2 += 1\n",
    "    possible_class = max(category0, category1, category2)\n",
    "    if(possible_class == category0):\n",
    "        pred_class = 0\n",
    "    elif(possible_class == category1):\n",
    "        pred_class = 1\n",
    "    else:\n",
    "        pred_class = 2 \n",
    "    \n",
    "    return  pred_class\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our classifier on the first sample of testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class:0\n",
      "actual class:0\n"
     ]
    }
   ],
   "source": [
    "pred_class = MultiNB_classify(x_test.getrow(0),theta, prior)\n",
    "\n",
    "print(\"predicted class:\" + str(pred_class))\n",
    "print(\"actual class:\" + str(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code below runs your classifier on every data sample from the testing dataset and stored them in \"y_pred\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(num_test):\n",
    "    pred_class = MultiNB_classify(x_test.getrow(i),theta=theta, prior= prior)\n",
    "    y_pred.append(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your result by comparing it with the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.916\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.87      0.92       389\n",
      "          1       0.92      0.93      0.93       394\n",
      "          2       0.85      0.95      0.90       251\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"accuracy: %0.3f\" % score)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the classification error (1-score) over the test set for various values of the smoothing parameter α and by trial and error find a good value of α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error value = 0.0841392649903\n"
     ]
    }
   ],
   "source": [
    "error = 1 - score\n",
    "print(\"Error value = \"+str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Error Obtained = 0.0841392649903\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "From Trail & Error,\n",
    "the best values of alpha observed are between 16.25 to 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
